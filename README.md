# ğŸ“š Book Scraper â€“ Crawler em Python com Requests-HTML

Um **crawler em Python** que coleta informaÃ§Ãµes de livros do site [Books to Scrape](https://books.toscrape.com/) e salva os dados em formato **JSON**.  

---

## ğŸš€ Funcionalidades

- ExtraÃ§Ã£o automatizada de:
  - TÃ­tulo do livro  
  - PreÃ§o  
  - Disponibilidade  
  - GÃªnero  
  - AvaliaÃ§Ã£o (estrelas)  
  - DescriÃ§Ã£o  
  - InformaÃ§Ãµes tÃ©cnicas (UPC, tipo de produto)  
  - URL do produto  
  - Data e hora da extraÃ§Ã£o  

- Armazenamento dos dados em um arquivo JSON bem formatado (`indent=4`, `ensure_ascii=False`).
- Suporte a mÃºltiplas pÃ¡ginas.

---

## ğŸ§  Tecnologias Utilizadas

| Categoria | Ferramenta |
|------------|-------------|
| Linguagem | Python 3.12 |
| Web Scraping | [`requests-html`](https://requests-html.kennethreitz.org/) |
| Ambiente virtual | [`pipenv`](https://pipenv.pypa.io/) |
| Empacotamento | Docker |
| PadrÃ£o de cÃ³digo | PEP8 |

---

## ğŸ“ Estrutura de Pastas

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Pipfile
â”œâ”€â”€ Pipfile.lock
â”œâ”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ books_data.json   # (gerado apÃ³s execuÃ§Ã£o)
â””â”€â”€ README.md
```

---

## âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o (sem Docker)

### 1ï¸âƒ£ Clonar o repositÃ³rio
```bash
git clone https://github.com/felipejbs/desafio-py-rpa.git
cd desafio-py-rpa
```

### 2ï¸âƒ£ Instalar o Pipenv
```bash
pip install pipenv
```

### 3ï¸âƒ£ Instalar as dependÃªncias
```bash
pipenv install
```

### 4ï¸âƒ£ Executar o projeto
```bash
pipenv run python main.py
```

### 5ï¸âƒ£ Ver o resultado
Os dados extraÃ­dos serÃ£o salvos no arquivo:
```
data/books_data.json
```

---

# ğŸ³ ExecuÃ§Ã£o com Docker

## 1. Build da imagem
```
docker build -t py-rpa-crawler .
```

## 2. Execute o container
```
docker run -d py-rpa-crawler
```

## ğŸ§© Estrutura do CÃ³digo

### `main.py`

O cÃ³digo Ã© modular e dividido em funÃ§Ãµes:

| FunÃ§Ã£o | DescriÃ§Ã£o |
|--------|------------|
| `get_book_data(book, session)` | Extrai os dados de um livro especÃ­fico. |
| `scrape_books(num_pages=5)` | Percorre as pÃ¡ginas e coleta os livros. |
| `save_books_data(books_data, filename)` | Salva os dados coletados em JSON. |
| `main()` | FunÃ§Ã£o principal que orquestra o processo. |

---

## ğŸ§± Exemplo de SaÃ­da (`data/books_data.json`)

```json
[
    {
        "title": "A Light in the Attic",
        "price": "Â£51.77",
        "genre": "Poetry",
        "availability": "In stock (22 available)",
        "avaliation": "Three",
        "description": "It's hard to imagine a world without A Light in the Attic...",
        "upc": "A897FE39",
        "product_type": "Books",
        "product_url": "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html",
        "extraction_datetime": "2025-10-27 17:45:30"
    }
]
```

---

## âœ¨ Boas PrÃ¡ticas Adotadas

- CÃ³digo conforme **PEP 8** (formataÃ§Ã£o, indentaÃ§Ã£o e nomes descritivos).
- Tipagem estÃ¡tica com *type hints* (`list[dict[str, str]]`).
- FunÃ§Ãµes pequenas e coesas.
- `ensure_ascii=False` para preservar acentuaÃ§Ã£o no JSON.

---

## ğŸ§ª Testes RÃ¡pidos

Para testar diferentes nÃºmeros de pÃ¡ginas:
```python
books_data = scrape_books(num_pages=5)
```

---

# Escolhas TÃ©cnicas e Trade-offs

## Linguagem e Bibliotecas

- **Python 3.12**  
  Escolhido por sua sintaxe moderna, tipagem opcional e ampla comunidade.
- **requests-html**  
  Permite scraping fÃ¡cil de pÃ¡ginas web, com parsing HTML e suporte a JavaScript simples.  
- **lxml**  
  Usado como backend para parsing HTML, Ã© rÃ¡pido e eficiente.

## Estrutura do Projeto

- **Script Ãºnico (`main.py`)**  
  MantÃ©m o projeto simples e fÃ¡cil de entender, ideal para projetos pequenos.
  **Trade-off:** Para projetos maiores, uma estrutura modular seria mais adequada.
- **Pasta `data/`**  
  Centraliza a saÃ­da dos dados, facilitando organizaÃ§Ã£o e acesso.

## Gerenciamento de DependÃªncias

- **Pipenv**  
  Facilita o gerenciamento de ambientes virtuais e dependÃªncias, garantindo reprodutibilidade.
  **Trade-off:** Pipenv pode ser mais lento que o uso direto de `pip` e `requirements.txt`, mas oferece melhor isolamento.

## ContainerizaÃ§Ã£o

- **Docker**  
  Garante que o projeto rode de forma idÃªntica em qualquer ambiente, evitando problemas de dependÃªncias de sistema.


## Scraping

- **Percorre pÃ¡ginas por URL sequencial**  
  Simples e eficiente para sites com estrutura previsÃ­vel.
  **Trade-off:** Se o site mudar a estrutura de URLs ou adicionar proteÃ§Ã£o anti-bot, o crawler pode quebrar facilmente.

## Robustez

- **Sem tratamento avanÃ§ado de erros**  
  O cÃ³digo assume que todas as pÃ¡ginas e elementos existem conforme esperado.
  **Trade-off:** Para uso em produÃ§Ã£o ou scraping de sites menos estÃ¡veis, seria necessÃ¡rio adicionar mais validaÃ§Ãµes e tratamento de exceÃ§Ãµes.

---

## Resumo

As escolhas priorizaram **simplicidade, clareza e facilidade de execuÃ§Ã£o** para um projeto de pequeno porte e propÃ³sito de exemplo.  
Para projetos maiores, mais robustos ou colaborativos, seria recomendado modularizar o cÃ³digo, e adicionar tratamento de erros

## ğŸ§° PossÃ­veis Melhorias Futuras

- Adicionar logs estruturados (`logging`).
- Armazenar os dados em banco SQL (SQLite ou PostgreSQL) ou NoSQL (DynamoDB, etc.).
- Implementar testes automatizados com `pytest`.

---

## ğŸ‘¨â€ğŸ’» Autor

**Felipe JerÃ´nimo Bernardo da Silva**  
ğŸ“§ [felipejeronimobs@gmail.com]
ğŸš€ Projeto desenvolvido como desafio de RPA/Web Scraping com Python.
